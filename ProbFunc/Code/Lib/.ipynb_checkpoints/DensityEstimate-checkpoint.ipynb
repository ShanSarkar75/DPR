{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "018b268a",
   "metadata": {},
   "source": [
    "# Set of Custom Functions for Density Estimations\n",
    "\n",
    "### Using only TensorFlow and Keras operations\n",
    "\n",
    "**Author:** S. Sarkar\n",
    "\n",
    "**Version:** 0.00\n",
    "\n",
    "**Release:** Aug/2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39343532",
   "metadata": {},
   "source": [
    "## SciPyKDE(X, bandwidth=None, s=None, Points)\n",
    "\n",
    "This function estimates the probability density function of a 1D dataset using SciPy’s Gaussian KDE with an adaptive bandwidth based on Silverman’s rule. It is build on NumPy and returns the evaluation points alongside the corresponding density values.\n",
    "\n",
    "## KDE( X,  Bandwidth=None, s=None, Points ):\n",
    "\n",
    "This function computes the kernel density estimate (KDE) of a 1D dataset using only TensorFlow and Keras operations, with a Gaussian kernel and adaptive bandwidth based on Silverman’s rule. It returns the estimated probability density over specified points efficiently within the TensorFlow graph.\n",
    "\n",
    "## HDE(S, nBins=None, Epsilon=1e-6):\n",
    "\n",
    "This function computes a normalized histogram density estimate for 1D data using only TensorFlow and Keras operations. It automatically selects the number of bins using the Freedman-Diaconis rule if not provided. It returns the bin centers, estimated density values, and bin width.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6108526",
   "metadata": {},
   "source": [
    "# Import Required Libraries \n",
    "\n",
    "**Keras Tenforflow**\n",
    "\n",
    "**SciPy**: Gaussian KDE Using SciPy API (For validation)\n",
    "\n",
    "**NumPy**: Python package for N-dimensional arrays (Used For SciPiKDE)\n",
    "\n",
    "### For Debugging\n",
    "\n",
    "**Matplotlib**: Python plotting library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79d7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False # Default Set it False\n",
    "# -----------------------------------------------------------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "# -----------------------------------------------------------------------------\n",
    "if DEBUG: import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1a1f74",
   "metadata": {},
   "source": [
    "# Gaussian KDE Using SciPy API\n",
    "\n",
    "This function computes a kernel density estimate (KDE) of a 1D dataset using the SciPy library’s implementation (scipy.stats.gaussian_kde). It applies Silverman’s rule to select the bandwidth if none is provided, then evaluates the estimated probability density function over a specified number of points spanning the data range. The function returns both the evaluation points and the corresponding density values.\n",
    "\n",
    "As the SciPy API functions are built on top of NumPy, we utilized NumPy functions when implementing the SciPyKDE() function\n",
    "\n",
    "Virtanen, P., Gommers, R., Oliphant, T.E. et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nat Methods 17, 261–272 (2020). https://doi.org/10.1038/s41592-019-0686-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59305a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Computes a 1D kernel density estimate (KDE) using SciPy's gaussian_kde.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like : Input data (Population).\n",
    "    bandwidth : str, scalar or callable, optional : Bandwidth for KDE. \n",
    "    If None: Using deafult Scott/Approx Silverman/Silverman.\n",
    "    \n",
    "    s: array-like :  Query Points (Default None).\n",
    "    Points : int : Number of points where the KDE is evaluated between min and max of X.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    s : ndarray : Points where the KDE is evaluated.\n",
    "    pdf : ndarray : Estimated density values at points x.\n",
    "    \n",
    "Note: \n",
    "If s is None using Points to estimate s using linspace \n",
    "If Scott is true it'll use Scott (1st pref)\n",
    "\"\"\"\n",
    "def SciPyKDE(X, bandwidth=None, s=None, Points=500, Scott=True, ApproxSilverman=True):\n",
    "    X = np.ravel(X).astype(np.float32)\n",
    "    N = X.size\n",
    "\n",
    "    # Bandwidth selection (Silverman's /Scotts rule if None)\n",
    "    # Approx Silverman - Assuming StdDev < IQR/1.34\n",
    "    # BW: ‘scott’, ‘silverman’ # Using Computed Silverman\n",
    "    if bandwidth is None:\n",
    "        if Scott: bandwidth='scott'\n",
    "        else:\n",
    "            # Using Degree of Freedom = 0 as assuming X is the entire population\n",
    "            #h = 1.06 min ( s , IQR / 1.34 ) n^(− 1 / 5)\n",
    "            # Assuming X is the entire population (ddof=0)\n",
    "            StdDev = np.std(X)  # Population standard deviation\n",
    "            if ApproxSilverman: bandwidth = 1.06 * StdDev * N**(-1/5) \n",
    "            else:\n",
    "                q75, q25 = np.percentile(X, [75 ,25])\n",
    "                iqr = q75 - q25; IQRFactor=iqr/1.34\n",
    "                # print(f\"IQR:{iqr:.3f} IQR/1.34={IQRFactor:.3f}\")\n",
    "                bandwidth = 1.06 * min(StdDev,IQRFactor) * N**(-1/5)        \n",
    "\n",
    "    if s is None:\n",
    "        # Evaluate KDE at Points between min and max of data\n",
    "        xMin, xMax = np.min(X), np.max(X)\n",
    "        s = np.linspace(xMin, xMax, Points)\n",
    "\n",
    "    # Use scipy KDE with bandwidth using Silverman's rule\n",
    "    kde = gaussian_kde(X, bw_method=bandwidth)\n",
    "    pdf = kde.evaluate(s)\n",
    "    return s, pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdad2ba",
   "metadata": {},
   "source": [
    "## For Evaluation / Debugging Purpose - SpiPyKDE Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05620cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************\n",
    "# Test Code\n",
    "#*************************************************************\n",
    "if DEBUG:\n",
    "    X_np = np.random.normal(loc=1.0, scale=1.5, size=10000)\n",
    "    # Run KDE\n",
    "    x_vals, pdf_vals = SciPyKDE(X_np, s=X_np[:1000])\n",
    "    print(f'X_np: {X_np.shape} x_vals: {x_vals.shape} pdf: {pdf_vals.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd98bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************\n",
    "# Test Code\n",
    "#*************************************************************\n",
    "if DEBUG:\n",
    "    def pSciPyKDE(X, bandwidth=None, Points=500, Scott=True, ApproxSilverman=True, Print=True):\n",
    "        # Flatten and cast to float32\n",
    "        X = np.ravel(X).astype(np.float32)\n",
    "        if Print: print(f\"1: X shape = {X.shape}\")\n",
    "\n",
    "        N = X.size\n",
    "        if Print: print(f\"2: N (number of samples) = {N}\")\n",
    "\n",
    "        if bandwidth is None:\n",
    "            if Scott: Bandwidth='scott'\n",
    "            else:\n",
    "                # Using Degree of Freedom = 0 as assuming X is the entire population\n",
    "                #h = 1.06 min ( s , IQR / 1.34 ) n^(− 1 / 5)\n",
    "                # Assuming X is the entire population (ddof=0)\n",
    "                StdDev = np.std(X)  # Population standard deviation\n",
    "                if Print: print(f\"3: StdDev = {StdDev}\")\n",
    "                if ApproxSilverman: bandwidth = 1.06 * StdDev * N**(-1/5) \n",
    "                else:\n",
    "                    q75, q25 = np.percentile(X, [75 ,25])\n",
    "                    iqr = q75 - q25; IQRFactor=iqr/1.34\n",
    "                    if Print: print(f\"3A: IQR:{iqr:.3f} IQR/1.34={IQRFactor:.3f}\")\n",
    "                    Bandwidth = 1.06 * min(StdDev,IQRFactor) * N**(-1/5) \n",
    "        else:\n",
    "            Bandwidth = bandwidth\n",
    "        if Print: print(f\"4: Using provided Bandwidth = {Bandwidth}\")\n",
    "\n",
    "        # Evaluate KDE at evenly spaced points between min and max of data\n",
    "        xMin, xMax = np.min(X), np.max(X)\n",
    "        if Print: \n",
    "            print(f\"5: xMin = {xMin}\")\n",
    "            print(f\"6: xMax = {xMax}\")\n",
    "\n",
    "        x = np.linspace(xMin, xMax, Points)\n",
    "        if Print: print(f\"7: x shape = {x.shape}\")\n",
    "\n",
    "        # Use SciPy gaussian_kde\n",
    "        kde = gaussian_kde(X, bw_method=Bandwidth)\n",
    "        pdf = kde.evaluate(x)\n",
    "        if Print: print(f\"8: pdf shape = {pdf.shape}\")\n",
    "\n",
    "        return x, pdf\n",
    "    #=====================================================================\n",
    "    X_np = np.random.normal(loc=1.0, scale=1.5, size=10000)\n",
    "    x_np, pdf_np = pSciPyKDE(X_np, Points=1000, Print=True)\n",
    "    #----------------------------------\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_np, pdf_np, lw=1, c='r', label='SciPyKDE')\n",
    "    #plt.plot(x, pdfSciPy, lw=1, ls='--', c='r', label='SciPy-KDE')\n",
    "    plt.hist(X_np, bins=500, density=True, alpha=0.4, color='gray', lw=0.5, label='Histogram')\n",
    "\n",
    "    plt.title('Kernel Density Estimation (SciPy-KDE)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08e770",
   "metadata": {},
   "source": [
    "# Kernel Density Estimation (KDE) – Gaussian Kernel\n",
    "\n",
    "Estimate a continuous probability density function (PDF) from a set of samples: $X = \\{x_1, x_2, \\dots, x_n\\}$\n",
    "\n",
    "### Kernel Density Estimate\n",
    "\n",
    "The estimated PDF at a point $x$ using kernel density estimation is:\n",
    "\n",
    "$\\hat{f}(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\left( \\frac{x - x_i}{h} \\right)$\n",
    "\n",
    "Where:\n",
    "- $n$: Number of samples  \n",
    "- $h$: Bandwidth (smoothing parameter)  \n",
    "- $K(u)$: Kernel function (we use Gaussian)\n",
    "\n",
    "---\n",
    "\n",
    "### Gaussian Kernel\n",
    "\n",
    "The Gaussian kernel is defined as:\n",
    "\n",
    "$K(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{u^2}{2} \\right)$\n",
    "\n",
    "Substituting into the KDE formula:\n",
    "\n",
    "$\\hat{f}(x) = \\frac{1}{n h \\sqrt{2\\pi}} \\sum_{i=1}^{n} \\exp\\left( -\\frac{(x - x_i)^2}{2 h^2} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "### Bandwidth Estimation\n",
    "\n",
    "If the bandwidth $h$ is not provided, we estimate it using **Silverman's rule of thumb**:\n",
    "\n",
    "$h = 1.06 \\cdot \\hat{\\sigma} \\cdot n^{-1/5}$\n",
    "\n",
    "Where $\\hat{\\sigma}$ is the standard deviation of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluation Grid\n",
    "\n",
    "To compute the estimated PDF on a grid:\n",
    "\n",
    "$z_j \\in [\\min(X), \\max(X)]$, for $j = 1, 2, \\dots, M$\n",
    "\n",
    "with $M$ being the number of points (typically $M = 500$).\n",
    "\n",
    "---\n",
    "\n",
    "### Final Estimation on Grid\n",
    "\n",
    "At each grid point $z_j$, the KDE becomes:\n",
    "\n",
    "$\\hat{f}(z_j) = \\frac{1}{n h \\sqrt{2\\pi}} \\sum_{i=1}^{n} \\exp\\left( -\\frac{(z_j - x_i)^2}{2 h^2} \\right)$\n",
    "\n",
    "---\n",
    "\n",
    "## **Memory Breakdown** for `KDE` Function\n",
    "\n",
    "\n",
    "| Tensor     | Shape  | Count |\n",
    "| ---------- | ------ | ----- |\n",
    "| `X_expand` | (N, 1) | N     |\n",
    "| `x_expand` | (1, M) | M     |\n",
    "| `diffSq`   | (N, M) | N×M   |\n",
    "| `Exp`      | (N, M) | N×M   |\n",
    "| `kernels`  | (N, M) | N×M   |\n",
    "| `pdf`      | (M,)   | M     |\n",
    "\n",
    "\n",
    "#### Total Memory (Worst Case)\n",
    "\n",
    "$ \\text{Total bytes} \\approx 12 N M + 4 N + 8 M $\n",
    "\n",
    "---\n",
    "\n",
    "#### In Mega Bytes (MB) / Kilo Bytes (KB)\n",
    "\n",
    "$ \\text{Total MB} \\approx \\frac{12 N M + 4 N + 8 M}{1024^2} $\n",
    "\n",
    "$\\text{Total KB} \\approx \\frac{12 N M + 4 N + 8 M}{1024} $\n",
    "\n",
    "\n",
    "Here, \n",
    "\n",
    "- $4$: Bytes per `float32`  \n",
    "- $N$: Number of samples  \n",
    "- $M$: Number of grid points\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc7296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# KDE: Kernel Density Estimation using Gaussian kernels (with TensorFlow)\n",
    "#\n",
    "# This function estimates the probability density function (PDF) of a 1D input\n",
    "# tensor `X` using Gaussian Kernel Density Estimation (KDE).\n",
    "# \n",
    "# - Bandwidth selection: If not provided, it uses Silverman's rule of thumb.\n",
    "# - Points: Number of grid points to evaluate the PDF over [min(X), max(X)].\n",
    "# - Computes pairwise differences and evaluates Gaussian kernel at each point.\n",
    "# - Uses @tf.function to compile and optimize the computation graph, with\n",
    "#   reduce_retracing=True to avoid retracing when input shapes remain constant.\n",
    "#\n",
    "# -----------\n",
    "# Parameters:\n",
    "# -----------\n",
    "#    X : array-like : Input data (Population).\n",
    "#    bandwidth : float, optional : Bandwidth for KDE.  If None: Using deafult Scott/Approx Silverman/Silverman.\n",
    "#    s: array-like :  Query Points (Default None).\n",
    "#    Points : int : Number of points where the KDE is evaluated between min and max of X.\n",
    "# -----------\n",
    "# Returns:\n",
    "# -----------\n",
    "#   s    : The evaluation grid (linspace from min to max of X).\n",
    "#   pdf  : The estimated probability density function over x.\n",
    "#\n",
    "# -----------\n",
    "# Note: If s is None using Points to estimate x using linspace\n",
    "# -----------\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "# Fewer retracings, thus better runtime as input varies\n",
    "@tf.function(reduce_retracing=True)\n",
    "# -----------------------------------------------------------------------------\n",
    "def KDE(X, Bandwidth=None, s=None, Points=500, Scott=True, ApproxSilverman=True):\n",
    "    # ----------------------------------------------\n",
    "    Pi = tf.acos(tf.constant(-1.0, dtype=tf.float32))\n",
    "    # ----------------------------------------------\n",
    "    # Reshapes X into a 1D tensor with shape (N,) and Get N\n",
    "    X = tf.cast(tf.reshape(X, [-1]), tf.float32) \n",
    "    N = tf.cast(tf.shape(X)[0], tf.float32)\n",
    "    # ----------------------------------------------\n",
    "    \n",
    "    # Bandwidth selection (Silverman's Scott's rule if None)\n",
    "    if Bandwidth is None:\n",
    "        # Using Degree of Freedom = 0 as assuming X is the entire population\n",
    "        # Assuming X is the entire population (ddof=0)\n",
    "        # h = 1.06 min ( s , IQR / 1.34 ) n − 1 / 5\n",
    "        # Approx Silverman - Assuming StdDev < IQR/1.34\n",
    "        # Equation 6.42. @Multivariate Density Estimation: Theory, Practice, and Visualization (1992)\n",
    "        # Scott's Rule:  s*n^(− 1 / [d+1]) -> d=1 ->  s*n^(− 0.2)\n",
    "        \n",
    "        StdDev = tf.math.reduce_std(X)\n",
    "        \n",
    "        if Scott: Bandwidth = StdDev * tf.pow(N, -0.2)        \n",
    "        else:  \n",
    "            if ApproxSilverman: Bandwidth = 1.06* StdDev * tf.pow(N, -0.2) \n",
    "            else:\n",
    "                # Sort values for quantile computation\n",
    "                X_sorted = tf.sort(X)\n",
    "                # Indices for 25th and 75th percentiles\n",
    "                idx_q25 = tf.cast(tf.floor(0.25 * (N - 1)), tf.int32)\n",
    "                idx_q75 = tf.cast(tf.floor(0.75 * (N - 1)), tf.int32)\n",
    "                q25 = X_sorted[idx_q25]\n",
    "                q75 = X_sorted[idx_q75]\n",
    "                # Interquartile range (IQR)\n",
    "                iqr = q75 - q25\n",
    "                IQRFactor = iqr / 1.34\n",
    "                # Bandwidth rule-of-thumb \n",
    "                Bandwidth = 1.06 * tf.minimum(StdDev, IQRFactor) * tf.pow(N, -0.2)\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    # s is a 1D tensor of shape (M,) — Query points\n",
    "    if s is None: # Using Points to estimate x using linspace\n",
    "        xMin, xMax = tf.reduce_min(X), tf.reduce_max(X)\n",
    "        S = tf.linspace(xMin, xMax, Points)\n",
    "    else: S = tf.cast(tf.identity(s), tf.float32)\n",
    "    # ----------------------------------------------\n",
    "    # Pairwise differences, normalized by bandwidth\n",
    "    X_Expand = tf.expand_dims(X, axis=1) # shape: (N, 1)\n",
    "    S_Expand = tf.expand_dims(S, axis=0) # shape: (1, M)\n",
    "    difSq = tf.square((S_Expand - X_Expand) / Bandwidth) # shape: (N, M)\n",
    "    # ----------------------------------------------\n",
    "    Exp=tf.exp(-0.5 * difSq) # shape: (N, M)\n",
    "    Kernels = Exp / (Bandwidth * tf.sqrt(2.0 * Pi)) # shape: (N, M)\n",
    "    pdf = tf.reduce_mean(Kernels, axis=0) # shape: (M)\n",
    "    # ----------------------------------------------\n",
    "    return S, pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02c5f5",
   "metadata": {},
   "source": [
    "## For Evaluation / Debugging Purpose - KDE Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01d70ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************\n",
    "# Test Code\n",
    "#*************************************************************\n",
    "if DEBUG:\n",
    "    X_np = np.random.normal(loc=1.0, scale=1.5, size=10000)\n",
    "    # Convert to Tensor\n",
    "    X_tf = tf.constant(X_np, dtype=tf.float32)\n",
    "    # Run KDE\n",
    "    x_vals, pdf_vals = KDE(X_tf, s=X_tf[:1000].numpy())\n",
    "    print(f'X_np: {X_np.shape} X_tf: {X_tf.shape}')\n",
    "    print(f'x_vals: {x_vals.shape} pdf: {pdf_vals.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2c9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************\n",
    "# Test Code\n",
    "#*************************************************************\n",
    "if DEBUG:\n",
    "    @tf.function#(reduce_retracing=True)\n",
    "    def pKDE(X, Bandwidth=None, Points=500, Scott= True, ApproxSilverman=True, Print=True):\n",
    "        # ----------------------------------------------\n",
    "        if Print:\n",
    "            def tfprint(name, value):\n",
    "                if isinstance(value, tf.Tensor):\n",
    "                    if value.shape.rank == 0:  # scalar tensor\n",
    "                        tf.print(f\"{name}: scalar =\", value)\n",
    "                    else:\n",
    "                        tf.print(f\"{name}: shape =\", tf.shape(value))\n",
    "                else:\n",
    "                    print(f\"{name}: value =\", value)\n",
    "        # ----------------------------------------------\n",
    "        Pi = tf.acos(tf.constant(-1.0, dtype=tf.float32)); \n",
    "        if Print: tfprint(\"Pi\", Pi)    \n",
    "        # ----------------------------------------------    \n",
    "        # Reshapes X into a 1D tensor with shape (N,) and Get N\n",
    "        X = tf.cast(tf.reshape(X, [-1]), tf.float32); \n",
    "        if Print: tfprint(\"X\", X) \n",
    "        N = tf.cast(tf.shape(X)[0], tf.float32); \n",
    "        if Print: tfprint(\"N\", N)\n",
    "        # ----------------------------------------------\n",
    "        # Bandwidth selection (Silverman's Scott's rule if None)\n",
    "        if Bandwidth is None:\n",
    "            # Using Degree of Freedom = 0 as assuming X is the entire population\n",
    "            # Assuming X is the entire population (ddof=0)\n",
    "            # h = 1.06 min ( s , IQR / 1.34 ) n − 1 / 5\n",
    "            # Approx Silverman - Assuming StdDev < IQR/1.34\n",
    "            # Equation 6.42. @Multivariate Density Estimation: Theory, Practice, and Visualization (1992)\n",
    "            # Scott's Rule:  s*n^(− 1 / [d+1]) -> d=1 ->  s*n^(− 0.2)\n",
    "\n",
    "            StdDev = tf.math.reduce_std(X)\n",
    "            if Print: tf.print(\"StdDev = \",StdDev)\n",
    "            if Scott: \n",
    "                tf.print(\"BW Type: Scott\")\n",
    "                Bandwidth = StdDev * tf.pow(N, -0.2)        \n",
    "            else:  \n",
    "                if ApproxSilverman: \n",
    "                    tf.print(\"BW Type: ApproxSilverman\")\n",
    "                    Bandwidth = 1.06* StdDev * tf.pow(N, -0.2) \n",
    "                else:\n",
    "                    tf.print(\"BW Type: ApproxSilverman\")\n",
    "                    # Sort values for quantile computation\n",
    "                    X_sorted = tf.sort(X)\n",
    "                    # Indices for 25th and 75th percentiles\n",
    "                    idx_q25 = tf.cast(tf.floor(0.25 * (N - 1)), tf.int32)\n",
    "                    idx_q75 = tf.cast(tf.floor(0.75 * (N - 1)), tf.int32)\n",
    "                    q25 = X_sorted[idx_q25]\n",
    "                    q75 = X_sorted[idx_q75]\n",
    "                    # Interquartile range (IQR)\n",
    "                    iqr = q75 - q25\n",
    "                    IQRFactor = iqr / 1.34\n",
    "                    if Print: tf.print(\"IQR: \",iqr,\" IQR/1.34= \" ,IQRFactor)\n",
    "                    # Bandwidth rule-of-thumb \n",
    "                    Bandwidth = 1.06 * tf.minimum(StdDev, IQRFactor) * tf.pow(N, -0.2)\n",
    "        else: tf.print(\"BW Type: Scalar\")\n",
    "        # ----------------------------------------------\n",
    "        if Print: tfprint(\"Bandwidth\", Bandwidth)\n",
    "        # ----------------------------------------------\n",
    "        # x is a 1D tensor of shape (M,) — Query points\n",
    "        xMin, xMax = tf.reduce_min(X), tf.reduce_max(X) \n",
    "        if Print: \n",
    "            tfprint(\"xMin\", xMin); \n",
    "            tfprint(\"xMax\", xMax)\n",
    "\n",
    "        x = tf.linspace(xMin, xMax, Points); \n",
    "        if Print: tfprint(\"x\", x)\n",
    "        # ----------------------------------------------\n",
    "        # Pairwise differences, normalized by bandwidth\n",
    "        X_Expand = tf.expand_dims(X, axis=1); \n",
    "        if Print: tfprint(\"X_Expand\", X_Expand)           # shape: (N, 1)\n",
    "        x_Expand = tf.expand_dims(x, axis=0); \n",
    "        if Print: tfprint(\"x_Expand\", x_Expand)           # shape: (1, M)\n",
    "        difSq = tf.square((x_Expand - X_Expand) / Bandwidth); \n",
    "        if Print: tfprint(\"difSq\", difSq)                 # shape: (N, M)\n",
    "        # ----------------------------------------------\n",
    "        Exp=tf.exp(-0.5 * difSq); \n",
    "        if Print: tfprint(\"Exp\", Exp)                     # shape: (N, M)\n",
    "        Kernels = Exp / (Bandwidth * tf.sqrt(2.0 * Pi)); \n",
    "        if Print: tfprint(\"Kernels\", Kernels)             # shape: (N, M)\n",
    "        pdf = tf.reduce_mean(Kernels, axis=0); \n",
    "        if Print:  tfprint(\"pdf\", pdf)                    # shape: (M)\n",
    "        # ----------------------------------------------\n",
    "        return x, pdf\n",
    "\n",
    "    #=====================================================================\n",
    "    X_np = np.random.normal(loc=1.0, scale=1.5, size=10000)\n",
    "    # Convert to Tensor\n",
    "    X_tf = tf.constant(X_np, dtype=tf.float32)\n",
    "    # Run KDE\n",
    "    x_vals, pdf_vals = pKDE(X_tf, Points=1000, Print=True)\n",
    "    # Convert output tensors to numpy for plotting\n",
    "    x_np = x_vals.numpy()\n",
    "    pdf_np = pdf_vals.numpy()\n",
    "    #----------------------------------\n",
    "    # Use scipy KDE with bandwidth using Silverman's rule\n",
    "    # Bandwidth selection (Silverman's rule if None)\n",
    "    x, pdfSciPy = SciPyKDE(X_np, bandwidth=None, Points=500)\n",
    "    #----------------------------------\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_np, pdf_np, lw=3, c='yellow', label='KDE')\n",
    "    plt.plot(x, pdfSciPy, lw=1, ls='--', c='r', label='SciPy-KDE')\n",
    "    plt.hist(X_np, bins=500, density=True, alpha=0.4, color='gray', lw=0.5, label='Histogram')\n",
    "\n",
    "    plt.title('Kernel Density Estimation (KDE)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc271909",
   "metadata": {},
   "source": [
    "# Histogram Density Estimation (HDE)\n",
    "\n",
    "Given a sample set \n",
    "$ S = \\{ s_1, s_2, \\ldots, s_n \\} $, we want to estimate the probability density function (PDF) by computing a histogram with bins determined by the Freedman-Diaconis rule.\n",
    "\n",
    "### Number of bins ($ n_{\\text{bins}} $)\n",
    "\n",
    "if not provided use the Freedman-Diaconis rule, to calculate the optimal number of bins $ n_{\\text{bins}} $ based on the sample size and variability in data.\n",
    "\n",
    "### Bin edges\n",
    "\n",
    "Define the bin edges as equally spaced points between the minimum and maximum sample values:\n",
    "\n",
    "$$\n",
    "\\text{edges} = \\{ e_0, e_1, \\ldots, e_{n_{\\text{bins}}} \\}, \\quad e_0 = \\min(S), \\quad e_{n_{\\text{bins}}} = \\max(S)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "e_i = \\min(S) + i \\cdot \\Delta, \\quad \\Delta = \\frac{\\max(S) - \\min(S)}{n_{\\text{bins}}}\n",
    "$$\n",
    "\n",
    "### Histogram counts\n",
    "\n",
    "For each bin $ i = 1, 2, \\ldots, n_{\\text{bins}} $, count the number of samples falling into the bin interval $ [e_{i-1}, e_i) $:\n",
    "\n",
    "$$\n",
    "c_i = \\text{number of samples } s_j \\text{ such that } e_{i-1} \\leq s_j < e_i\n",
    "$$\n",
    "\n",
    "The total number of samples is:\n",
    "\n",
    "$$\n",
    "N = \\sum_{i=1}^{n_{\\text{bins}}} c_i = n\n",
    "$$\n",
    "\n",
    "### Bin width\n",
    "\n",
    "The bin width is:\n",
    "\n",
    "$$\n",
    "h = \\frac{\\max(S) - \\min(S)}{n_{\\text{bins}}}\n",
    "$$\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "The histogram-based density estimate at bin $ i $ is: $\\hat{f}(x_i) = \\frac{c_i}{N \\cdot h}$\n",
    "\n",
    "where $ x_i $ is the bin center: $x_i = \\frac{e_{i-1} + e_i}{2}$\n",
    "\n",
    "### Gaussian Filtering for Smoothing\n",
    "\n",
    "Applied **Gaussian filtering** on the raw histogram density estimate $\\hat{f}(x)$ to produce a smoother estimate.\n",
    "\n",
    "#### Gaussian kernel\n",
    "\n",
    "Utilized a 1D Normalized Gaussian kernel of size $m$ and Smoothing Parameter $h$ is\n",
    "\n",
    "$$\n",
    "g_k = \\frac{\\exp\\left(-\\frac{1}{2} \\left(\\frac{k}{h}\\right)^2\\right)}{\\sum_{j=-\\lfloor m/2 \\rfloor}^{\\lfloor m/2 \\rfloor} \\exp\\left(-\\frac{1}{2} \\left(\\frac{j}{h}\\right)^2\\right)},  \n",
    "\\quad k \\in \\left[-\\lfloor m/2 \\rfloor, \\ldots, \\lfloor m/2 \\rfloor\\right]\n",
    "$$\n",
    "\n",
    "Here, \n",
    "\n",
    "- $\\sum_k g_k = 1$ \n",
    "- $\\text{Kernel size}\\ (m)\\ = \\max\\left(f_{\\text{min}},\\; \\left\\lfloor 6\\sigma \\right\\rfloor \\;\\right)$ \n",
    "- $f_{\\text{min}}\\ = 5\\ (\\text{default})$ \n",
    "- $\\text{Smoothing Parameter}\\ (h) = 2.5 (\\text{default})$ \n",
    "\n",
    "\n",
    "#### Convolution\n",
    "\n",
    "The smoothed density is obtained via\n",
    "\n",
    "$$\n",
    "\\tilde{f}(x_i) = \\sum_{k=-\\lfloor m/2 \\rfloor}^{\\lfloor m/2 \\rfloor} g_k \\cdot \\hat{f}(x_{i-k})\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Freedman–Diaconis Rule for Histogram Binning\n",
    "\n",
    "The **Freedman–Diaconis rule** is a robust, data-driven method to determine the optimal bin width for a histogram that approximates the **probability density function (PDF)** of a continuous variable.\n",
    "\n",
    "**The Freedman–Diaconis rule is:** $h = 2 \\cdot \\frac{\\text{IQR}}{n^{1/3}}$\n",
    "\n",
    "Here,\n",
    "\n",
    "- $n$: number of samples  \n",
    "- $Q_1$: 25th percentile (first quartile)  \n",
    "- $Q_3$: 75th percentile (third quartile)  \n",
    "- $\\text{IQR} = Q_3 - Q_1$: interquartile range (IQR)  \n",
    "- $h$: optimal bin width  \n",
    "\n",
    "\n",
    "#### Number of Bins\n",
    "\n",
    "Let $\\text{Range} = \\max(X) - \\min(X)$, then the number of bins $k$ is:\n",
    "\n",
    "$$\n",
    "k = \\left\\lceil \\frac{\\text{Range}}{h} \\right\\rceil \n",
    "= \\left\\lceil \\frac{\\max(X) - \\min(X)}{2 \\cdot \\text{IQR} / n^{1/3}} \\right\\rceil\n",
    "$$\n",
    "\n",
    "\n",
    "#### Reference\n",
    "\n",
    "Freedman, David, and Persi Diaconis. \"On the Histogram as a Density Estimator: L2 Theory.\" Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete, vol. 57, no. 4, 1981, pp. 453–476.\n",
    "\n",
    "---\n",
    "\n",
    "## **Memory Breakdown** for `HDE` Function\n",
    "\n",
    "| **Tensor** |  **Shape** |**Count**|     **Description**    |\n",
    "|:----------:|:----------:|:-------:|:----------------------:|\n",
    "| S          | ( N, )     | $N$     | Input vector           |\n",
    "| X\\_sorted  | ( N, )     | $N$     | Sorted values          |\n",
    "| edges      | ( M+1, )   | $M{+}1$ | Bin edges              |\n",
    "| counts     | ( M, )     | $M$     | Bin counts (int)       |\n",
    "| centers    | ( M, )     | $M$     | Bin centers            |\n",
    "| density    | ( M, )     | $M$     | Normalized density     |\n",
    "| x          | ( 1, M, 1 )| $M$     | Reshaped density       |\n",
    "| kernel     | ( S, 1, 1 )| $S$     | Gaussian Kernel (S<<M) |\n",
    "| xFiltered  | ( 1, M, 1 )| $M$     | Filtered Output        |\n",
    "| pdf        | ( M, )     | $M$     | Probabilty density     |\n",
    "\n",
    "\n",
    "#### Total Memory ( Estimation )\n",
    "\n",
    "$ \\text{Total bytes} \\approx 4 \\cdot (2N + 7M) $\n",
    "\n",
    "Ignored $S\\ ( Kernel\\ size)$ and and approximated $M+1\\approx M$\n",
    "\n",
    "---\n",
    "\n",
    "#### In Mega Bytes (MB) / Kilo Bytes (KB)\n",
    "\n",
    "$ \\text{Total MB} \\approx \\frac{4 \\cdot (2N + 7M)}{1024^2} $\n",
    "\n",
    "$\\text{Total KB} \\approx \\frac{4 \\cdot (2N + 7M)}{1024} $\n",
    "\n",
    "\n",
    "Here, \n",
    "\n",
    "- $4$: Bytes per `float32`  \n",
    "- $N$: Number of samples  \n",
    "- $M$: Number of grid points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84505532",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "# Function: Freedman-Diaconis Rule for Bins\n",
    "#----------------------------------------------------\n",
    "# Fewer retracings, thus better runtime as input varies\n",
    "@tf.function(reduce_retracing=True)\n",
    "def BinsFreedmanDiaconisRule(X):\n",
    "    n = tf.cast(tf.size(X), tf.float32)  # Scalar\n",
    "\n",
    "    X_sorted = tf.sort(X)  # Shape: (N,), sorted samples\n",
    "\n",
    "    idx_25 = tf.cast(tf.round(0.25 * (n - 1)), tf.int32)  # Scalar index\n",
    "    idx_75 = tf.cast(tf.round(0.75 * (n - 1)), tf.int32)  # Scalar index\n",
    "\n",
    "    q25 = X_sorted[idx_25]  # Scalar (25th percentile)\n",
    "    q75 = X_sorted[idx_75]  # Scalar (75th percentile)\n",
    "\n",
    "    iqr = q75 - q25  # Scalar (interquartile range)\n",
    "    bin_width = 2.0 * iqr / tf.pow(n, 1.0 / 3.0)  # Scalar (bin width)\n",
    "    if bin_width == 0: bin_width = 1e-12 # Fail safe\n",
    "    range_X = tf.reduce_max(X) - tf.reduce_min(X)  # Scalar (range)\n",
    "\n",
    "    bins = tf.math.ceil(range_X / bin_width) # Scalar (number of bins)\n",
    "\n",
    "    return tf.cast(bins, tf.int32)  # Scalar (int)\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Gaussian Kernel For Gaussian Filtering \n",
    "#--------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def GaussianKernel(size, Sigma=2.5):\n",
    "    x = tf.range(-size // 2 + 1, size // 2 + 1, dtype=tf.float32)\n",
    "    kernel = tf.exp(-0.5 * (x / Sigma) ** 2)\n",
    "    kernel /= tf.reduce_sum(kernel)\n",
    "    return tf.reshape(kernel, [size, 1, 1])  # [filter_width, in_channels, out_channels]\n",
    "#--------------------------------------------------------\n",
    "# Gaussian Filter to Smooth the Histogram Density Estimation\n",
    "#--------------------------------------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def GaussianFilter(x, fSizeMin=5, Sigma=2.5):\n",
    "    Stddev = tf.math.reduce_std(x)               # sample stddev \n",
    "    val = 6.0 * Stddev                           # tensor multiplication\n",
    "    val_int = tf.cast(tf.floor(val), tf.int32)   # floor and cast to int tensor\n",
    "    val_odd = val_int | 1                        # bitwise OR to make it odd\n",
    "    Size = tf.maximum(fSizeMin, val_odd)         # ensure minimum size fSizeMin\n",
    "\n",
    "    x = tf.reshape(x, [1, -1, 1])\n",
    "    kernel = GaussianKernel(Size, Sigma)\n",
    "    xFiltered = tf.nn.conv1d(x, kernel, stride=1, padding='SAME')\n",
    "    return tf.squeeze(xFiltered)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4cc7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fewer retracings, thus better runtime as input varies\n",
    "@tf.function(reduce_retracing=True)\n",
    "# -----------------------------------------------------------------------------\n",
    "def HDE(S, nBins=None, Epsilon=1e-12, fSizeMin=5, fSigma=2.5):\n",
    "    S = tf.cast(tf.reshape(S, [-1]), tf.float32)  # Shape: (N,), flatten input samples\n",
    "    #--------------------------------------------------------\n",
    "    # Compute No of Bins Freedman-Diaconis Rule If nBins=None Else use provided nBins as-is\n",
    "    #-------------------------------------------------------- \n",
    "    if nBins is None: nBins = BinsFreedmanDiaconisRule(S)  # Scalar (number of bins)\n",
    "        \n",
    "    #--------------------------------------------------------\n",
    "    # Compute histogram\n",
    "    #--------------------------------------------------------\n",
    "    nBins = tf.maximum(nBins, 1)  # Ensure at least 1 bin, scalar\n",
    "    \n",
    "    min_S = tf.reduce_min(S)  # Scalar (minimum value)\n",
    "    max_S = tf.reduce_max(S)  # Scalar (maximum value)\n",
    "    \n",
    "    edges = tf.linspace(min_S, max_S, nBins + 1)  # Shape: (nBins+1,), bin edges\n",
    "    \n",
    "    counts = tf.histogram_fixed_width(S, [min_S, max_S], nbins=nBins) # Shape: (nBins,), count of samples/bin\n",
    "    \n",
    "    bin_width = (max_S - min_S) / tf.cast(nBins, tf.float32)  # Scalar (width of each bin)\n",
    "    \n",
    "    total = tf.reduce_sum(counts)  # Scalar (total number of samples)\n",
    "    \n",
    "    density = tf.cast(counts, tf.float32) / (tf.cast(total, tf.float32) * bin_width) # Shape: (nBins,), normalized density per bin\n",
    "    pdf=GaussianFilter(density, fSizeMin, fSigma)\n",
    "    centers = (edges[:-1] + edges[1:]) / 2.0  # Shape: (nBins,), bin centers\n",
    "    return centers, pdf, float(bin_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c80481",
   "metadata": {},
   "source": [
    "## For Evaluation / Debugging Purpose - HDE Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88060700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************\n",
    "# Test Code\n",
    "#*************************************************************\n",
    "if DEBUG:\n",
    "    @tf.function#(reduce_retracing=True)\n",
    "    def pHDE(S, nBins=None, Epsilon=1e-12, fSizeMin=5, fSigma=2.5, Print=True):\n",
    "        #--------------------------------------------------------\n",
    "        # Function to print Tensor\n",
    "        #--------------------------------------------------------       \n",
    "        def tfprint(name, value):\n",
    "            if isinstance(value, tf.Tensor):\n",
    "                if value.shape.rank == 0:  # scalar tensor\n",
    "                    tf.print(f\"{name}: scalar =\", value)\n",
    "                else:\n",
    "                    tf.print(f\"{name}: shape =\", tf.shape(value))\n",
    "            else:\n",
    "                print(f\"{name}: value =\", value)\n",
    "        \n",
    "        #--------------------------------------------------------\n",
    "        S = tf.cast(tf.reshape(S, [-1]), tf.float32)  # Shape: (N,), flatten input samples\n",
    "        if Print: tfprint(\"S\", S)\n",
    "        #--------------------------------------------------------\n",
    "        # Compute No of Bins Freedman-Diaconis Rule If nBins=None Else use provided nBins as-is\n",
    "        #-------------------------------------------------------- \n",
    "        if nBins is None: \n",
    "            #----------------------------------------------------\n",
    "            # Inner Function: Freedman-Diaconis Rule for Bins\n",
    "            #----------------------------------------------------\n",
    "            def BinsFreedmanDiaconisRule(X):\n",
    "                n = tf.cast(tf.size(X), tf.float32)  # Scalar\n",
    "                if Print: tfprint(\"n (sample size)\", n)\n",
    "\n",
    "                X_sorted = tf.sort(X)  # Shape: (N,), sorted samples\n",
    "                if Print: tfprint(\"X_sorted\", X_sorted)\n",
    "\n",
    "                idx_25 = tf.cast(tf.round(0.25 * (n - 1)), tf.int32)  # Scalar index\n",
    "                idx_75 = tf.cast(tf.round(0.75 * (n - 1)), tf.int32)  # Scalar index\n",
    "                if Print: tfprint(\"idx_25\", idx_25)\n",
    "                if Print: tfprint(\"idx_75\", idx_75)\n",
    "\n",
    "                q25 = X_sorted[idx_25]  # Scalar (25th percentile)\n",
    "                q75 = X_sorted[idx_75]  # Scalar (75th percentile)\n",
    "                if Print: tfprint(\"q25\", q25)\n",
    "                if Print: tfprint(\"q75\", q75)\n",
    "\n",
    "                iqr = q75 - q25  # Scalar (interquartile range)\n",
    "                if Print: tfprint(\"iqr\", iqr)\n",
    "\n",
    "                bin_width = 2.0 * iqr / tf.pow(n, 1.0 / 3.0)  # Scalar (bin width)\n",
    "                if tf.equal(bin_width, 0.0):\n",
    "                    bin_width = Epsilon  # Fail safe\n",
    "                if Print: tfprint(\"bin_width\", bin_width)\n",
    "\n",
    "                range_X = tf.reduce_max(X) - tf.reduce_min(X)  # Scalar (range)\n",
    "                if Print: tfprint(\"range_X\", range_X)\n",
    "\n",
    "                bins = tf.math.ceil(range_X / bin_width)  # Scalar (number of bins)\n",
    "                if Print: tfprint(\"bins (nBins)\", bins)\n",
    "\n",
    "                return tf.cast(bins, tf.int32)  # Scalar (int)\n",
    "            #----------------------------------------------------\n",
    "            nBins = BinsFreedmanDiaconisRule(S)  # Scalar (number of bins)\n",
    "        #--------------------------------------------------------\n",
    "        nBins = tf.maximum(nBins, 1)  # Ensure at least 1 bin, scalar   \n",
    "        if Print: tfprint(\"nBins final\", nBins)        \n",
    "        #--------------------------------------------------------\n",
    "        # Gaussian Kernel For Gaussian Filtering \n",
    "        #--------------------------------------------------------\n",
    "        def GaussianKernel(size, Sigma=fSigma):\n",
    "            x = tf.range(-size // 2 + 1, size // 2 + 1, dtype=tf.float32)\n",
    "            if Print: tfprint(\"GaussianKernel - x\", x) \n",
    "            kernel = tf.exp(-0.5 * (x / Sigma) ** 2)\n",
    "            kernel /= tf.reduce_sum(kernel)\n",
    "            if Print: tfprint(\"GaussianKernel - kernel\", kernel) \n",
    "            return tf.reshape(kernel, [size, 1, 1])  # [filter_width, in_channels, out_channels]\n",
    "        #--------------------------------------------------------\n",
    "        # Gaussian Filter to Smooth the Histogram Density Estimation\n",
    "        #--------------------------------------------------------\n",
    "        def GaussianFilter(x, Sigma=fSigma):\n",
    "            Stddev = tf.math.reduce_std(x)               # sample stddev \n",
    "            if Print: tfprint(\"GaussianFilter - Stddev\", Stddev)\n",
    "            val = 6.0 * Stddev                           # tensor multiplication\n",
    "            val_int = tf.cast(tf.floor(val), tf.int32)   # floor and cast to int tensor\n",
    "            val_odd = val_int | 1                        # bitwise OR to make it odd\n",
    "            if Print: tf.print(\"val:\", val, \" val_int:\", val_int, \" val_odd:\", val_odd)\n",
    "            Size = tf.maximum(fSizeMin, val_odd)         # ensure minimum size fSizeMin\n",
    "            if Print: tf.print(\"GaussianFilter - Size\", Size) \n",
    "            x = tf.reshape(x, [1, -1, 1])\n",
    "            if Print: tfprint(\"GaussianFilter - x\", x) \n",
    "            kernel = GaussianKernel(Size, Sigma)\n",
    "            if Print: tfprint(\"GaussianFilter - kernel\", kernel) \n",
    "            xFiltered = tf.nn.conv1d(x, kernel, stride=1, padding='SAME')\n",
    "            if Print: tfprint(\"GaussianFilter - xFiltered\", xFiltered) \n",
    "            return tf.squeeze(xFiltered)    \n",
    "        #--------------------------------------------------------\n",
    "        # Compute histogram\n",
    "        #--------------------------------------------------------        \n",
    "        min_S = tf.reduce_min(S)  # Scalar (minimum value)\n",
    "        max_S = tf.reduce_max(S)  # Scalar (maximum value)\n",
    "        if Print:\n",
    "            tfprint(\"min_S\", min_S)\n",
    "            tfprint(\"max_S\", max_S)\n",
    "\n",
    "        edges = tf.linspace(min_S, max_S, nBins + 1)  # Shape: (nBins+1,), bin edges\n",
    "        if Print: tfprint(\"edges\", edges)\n",
    "\n",
    "        counts = tf.histogram_fixed_width(S, [min_S, max_S], nbins=nBins)  # Shape: (nBins,), count of samples/bin\n",
    "        if Print: tfprint(\"counts\", counts)\n",
    "\n",
    "        bin_width = (max_S - min_S) / tf.cast(nBins, tf.float32)  # Scalar (width of each bin)\n",
    "        if Print: tfprint(\"bin_width\", bin_width)\n",
    "\n",
    "        total = tf.reduce_sum(counts)  # Scalar (total number of samples)\n",
    "        if Print: tfprint(\"total\", total)\n",
    "\n",
    "        density = tf.cast(counts, tf.float32) / (tf.cast(total, tf.float32) * bin_width)  # Shape: (nBins,), normalized density per bin\n",
    "        if Print: tfprint(\"density\", density)\n",
    "            \n",
    "        pdf=GaussianFilter(density)\n",
    "        if Print: tfprint(\"pdf\", pdf)\n",
    "        \n",
    "        centers = (edges[:-1] + edges[1:]) / 2.0  # Shape: (nBins,), bin centers\n",
    "        if Print: tfprint(\"centers\", centers)\n",
    "\n",
    "        return centers, pdf, float(bin_width)\n",
    "\n",
    "    #=====================================================================\n",
    "    X_np = np.random.normal(loc=1.0, scale=1.5, size=10000)\n",
    "    # Convert to Tensor\n",
    "    X_tf = tf.constant(X_np, dtype=tf.float32)\n",
    "    # Run histogram density estimation with print enabled\n",
    "    x_vals, pdf_vals, bin_width = pHDE(X_tf, Print=True)    \n",
    "    # Convert output tensors to numpy for plotting\n",
    "    x_np = x_vals.numpy()\n",
    "    pdf_np = pdf_vals.numpy()\n",
    "    #----------------------------------\n",
    "    # Use scipy KDE with bandwidth using Silverman's rule\n",
    "    # Bandwidth selection (Silverman's rule if None)\n",
    "    x, pdfSciPy = SciPyKDE(X_np, bandwidth=None, Points=500)\n",
    "    #----------------------------------\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x_np, pdf_np, lw=3, c='yellow', label='HDE')\n",
    "    plt.plot(x, pdfSciPy, lw=1, ls='--', c='r', label='SciPy-KDE')\n",
    "    plt.hist(X_np, bins=500, density=True, alpha=0.4, color='gray', lw=0.5, label='Histogram')\n",
    "\n",
    "    plt.title('Density Estimation Using HDE')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf02067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
